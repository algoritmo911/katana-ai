# Аудит и План Рефакторинга Когнитивного Ядра NLP

**Дата:** 2025-09-17
**Аудитор:** Jules
**Версия:** 1.0

## 1. Общая оценка

Текущая реализация Когнитивного Ядра является функциональным прототипом. Она успешно демонстрирует возможность интеграции с OpenAI и обработки диалогов. Однако архитектура не соответствует требованиям производственной системы по надежности, масштабируемости и поддерживаемости. Код содержит "грязные" участки, которые необходимо устранить.

**Основная проблема:** Процедурный стиль с глобальным состоянием (`user_memory`) и монолитные функции с размытой ответственностью.

## 2. Анализ по модулям и план действий

### 2.1. `bot/katana_bot.py`

*   **Проблема 1: Глобальное состояние.** Глобальный словарь `user_memory` делает систему непредсказуемой, трудно тестируемой и не потокобезопасной.
    *   **Решение:** Инкапсулировать всю логику бота в новый класс `KatanaBot`. `user_memory` станет его полем (`self.sessions` или `self.dialogue_manager`). Это позволит управлять состоянием централизованно.

*   **Проблема 2: Монолитная функция `_generate_response`.** Функция смешивает в себе логику выбора приоритетного намерения (приветствие, фрейм, основной интент) и вызов соответствующего обработчика.
    *   **Решение:** Декомпозировать. Создать отдельную функцию `_select_primary_intent`, которая будет содержать логику выбора. `_generate_response` будет только вызывать ее, а затем вызывать нужный хендлер.

*   **Проблема 3: Монолитная функция `_update_state`.** Смешивает обновление контекста, истории и логирование метрик.
    *   **Решение:** Разделить на три приватных метода: `_update_dialogue_context`, `_update_history`, `_log_turn_metrics`.

*   **Проблема 4: Отсутствие роутера.** `INTENT_HANDLERS` — это простой словарь.
    *   **Решение:** Создать класс `IntentRouter`, который будет инкапсулировать `INTENT_HANDLERS` и иметь метод `.get_handler(intent_name)`. Это позволит в будущем легко добавлять более сложную логику маршрутизации.

### 2.2. `bot/nlp/parser.py`

*   **Проблема 1: Размытая ответственность.** Функция `analyze_text` занимается не только парсингом, но и управлением контекстом (слияние сущностей). Это главная архитектурная ошибка.
    *   **Решение:** Полностью удалить логику слияния контекста из парсера. Его задача — только адаптировать "сырой" JSON от `NLPProcessor` в структурированный объект `NLPResult` (например, dataclass), не изменяя его на основе предыдущего контекста.

*   **Проблема 2: Монолитность.** Функция `analyze_text` выполняет несколько разных преобразований.
    *   **Решение:** Разделить на более мелкие, чистые функции: `_adapt_intent`, `_adapt_entities`.

*   **Проблема 3: Жестко закодированные правила.** `intent_map` и проверка на "погоду" должны быть вынесены.
    *   **Решение:** `intent_map` должен стать частью конфигурации или отдельного модуля. Проверка на "погоду" должна быть удалена в пользу более точного промпта для OpenAI.

### 2.3. `bot/nlp/context.py`

*   **Проблема 1: Неверное распределение ответственности.** Модуль почти ничего не делает, так как основная логика "утекла" в `parser.py`.
    *   **Решение:** Вернуть логику управления контекстом сюда. Создать класс `DialogueContextManager`. Его метод `update_context` должен будет принимать старый контекст и новый `NLPResult` и решать, как их объединять (например, на основе `dialogue_state`).

*   **Проблема 2: Структура контекста.** Начальный контекст содержит ключ `history`, который не используется.
    *   **Решение:** Структура контекста должна быть пересмотрена. Она должна явно содержать `entities` и, возможно, краткую сводку последних реплик, но не всю историю.

### 2.4. `bot/nlp/nlp_processor.py`

*   **Проблема 1: Монолитный промпт.** Системный промпт находится прямо в коде, что затрудняет его изменение.
    *   **Решение:** Вынести промпт в отдельный файл, например, `system_prompt.md`. `NLPProcessor` будет читать его при инициализации.

*   **Проблема 2: Жестко закодированные параметры.** Имя модели (`gpt-4o`) и температура (`0`) жестко заданы.
    *   **Решение:** Вынести в конфигурационный файл или переменные окружения.

*   **Проблема 3: Отсутствие кэширования.** Каждый вызов идет в API, что медленно и дорого.
    *   **Решение:** Добавить кэширование для метода `process_text`. Можно начать с простого `functools.lru_cache`, а в перспективе использовать Redis для персистентного кэша.

## 3. Итоговый План Рефакторинга

1.  **Создать классы:** `KatanaBot`, `IntentRouter`, `DialogueContextManager`, `NLPProcessorAdapter` (новый `parser`).
2.  **Перенести логику:** Переместить `user_memory` и функции-пайплайны в `KatanaBot`. Переместить логику слияния сущностей из `parser.py` в `DialogueContextManager`.
3.  **Декомпозировать:** Разбить `_generate_response`, `_update_state`, `analyze_text` на более мелкие функции.
4.  **Вынести конфигурацию:** Промпт и параметры модели вынести из кода `NLPProcessor`.
5.  **Добавить кэширование:** Реализовать `lru_cache` для `NLPProcessor.process_text`.
6.  **Обновить тесты:** Адаптировать все тесты под новую, классо-ориентированную архитектуру.
