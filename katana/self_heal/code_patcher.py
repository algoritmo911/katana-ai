import os
from bot.nlp_clients.openai_client import OpenAIClient

class CodePatcher:
    """
    Generates a code patch to fix an error based on an analysis.
    """

    def __init__(self):
        self.nlp_client = OpenAIClient()

    def generate_patch(self, analysis: dict) -> dict:
        """
        Generates a code patch.

        Args:
            analysis: The analysis dictionary from the FailureAnalyzer.
                      Expected keys: 'file', 'line', 'root_cause_hypothesis'.

        Returns:
            A dictionary containing the original code snippet and the proposed patch.
        """
        file_path = analysis.get("file")
        line_num = analysis.get("line")
        hypothesis = analysis.get("root_cause_hypothesis")

        if not all([file_path, line_num, hypothesis]):
            return {"error": "Invalid analysis format provided."}

        if not os.path.exists(file_path):
            return {"error": f"File not found: {file_path}"}

        # Read the problematic code snippet
        original_snippet = self._get_code_snippet(file_path, line_num)

        if not original_snippet:
            return {"error": f"Could not read code snippet from {file_path} at line {line_num}."}

        # Formulate a prompt for the LLM
        prompt = self._create_llm_prompt(original_snippet, hypothesis, file_path)

        # Get the patch from the LLM
        try:
            new_patch = self.nlp_client.generate_text(prompt)
            cleaned_patch = self._clean_patch(new_patch)
            return {
                "original_snippet": original_snippet,
                "patched_snippet": cleaned_patch,
            }
        except Exception as e:
            return {"error": f"Failed to generate patch from LLM: {e}"}

    def _get_code_snippet(self, file_path: str, line_num: int, context_lines: int = 10) -> str:
        """
        Extracts a snippet of code centered around the given line number.
        """
        with open(file_path, "r") as f:
            lines = f.readlines()

        start = max(0, line_num - context_lines - 1)
        end = min(len(lines), line_num + context_lines)

        return "".join(lines[start:end])

    def _create_llm_prompt(self, code_snippet: str, hypothesis: str, file_path: str) -> str:
        """
        Creates a prompt for the LLM to generate a patch.
        """
        prompt = (
            "You are an expert Python developer. Your task is to fix a bug in a Python file.\n"
            f"The file is: `{file_path}`\n\n"
            "Here is the root cause hypothesis for the bug:\n"
            f"'{hypothesis}'\n\n"
            "Here is the relevant code snippet:\n"
            "```python\n"
            f"{code_snippet}\n"
            "```\n\n"
            "Based on this information, please provide a code patch to fix the bug.\n"
            "Follow these instructions carefully:\n"
            "1. Only output the new, corrected version of the code snippet.\n"
            "2. Do not include any explanations, comments, or markdown formatting like ```python.\n"
            "3. Ensure the corrected code is syntactically correct and production-ready.\n"
            "4. Add detailed logging to explain the fix.\n"
            "5. If necessary, add a try-except block to handle the error gracefully.\n\n"
            "Corrected Code:\n"
        )
        return prompt

    def _clean_patch(self, patch: str) -> str:
        """
        Cleans the patch generated by the LLM.
        Removes markdown code blocks and leading/trailing whitespace.
        """
        if patch.startswith("```python"):
            patch = patch[len("```python"):]
        if patch.endswith("```"):
            patch = patch[:-len("```")]
        return patch.strip()
